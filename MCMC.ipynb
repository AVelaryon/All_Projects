{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, count\n",
    "from numpy.linalg import det, inv, norm\n",
    "from dsci_project_assignment import Gradient_Descent\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import multivariate_normal as multi_n\n",
    "from scipy.stats import uniform as uni\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "# Fetching Data:\n",
    "data  = fetch_california_housing(as_frame=True)['frame'].copy()\n",
    "data = np.c_[np.ones((data.shape[0],1)),data.to_numpy()]\n",
    "minmax = MinMaxScaler()\n",
    "transformer = QuantileTransformer(n_quantiles=5000, output_distribution='uniform', random_state=0)\n",
    "data[:,:-1] =transformer.fit_transform(data[:,:-1],data[:,-1])\n",
    "\n",
    "# Train/Val/Test Split:\n",
    "val = train_test_split(data[:,:-1],data[:,-1], train_size=0.8, random_state=0)\n",
    "\n",
    "\n",
    "# Kernel:\n",
    "def kernel(data: np.ndarray):\n",
    "    rows,columns= data.shape\n",
    "    K = np.zeros((columns,columns))\n",
    "    for i in range(columns):\n",
    "        for j in range(columns):\n",
    "            K[i,j] = np.exp((-1/2)*norm(data[:,i]-data[:, j])**2)\n",
    "    return K\n",
    "def gauss_pdf(x, mu, sig):\n",
    "    exp = np.exp(-pow(x-mu, 2)/(2*pow(sig,2)))\n",
    "    return 1/(np.sqrt(2*np.pi)*sig)*exp\n",
    "def gaussian_kernel(u):\n",
    "    '''\n",
    "    Kernel Density Function\n",
    "    '''\n",
    "    return 1/(np.sqrt(2*np.pi))*np.exp(-0.5*pow(u,2))\n",
    "def kde(x, data, bandwidth):\n",
    "    n = len(data)\n",
    "    estimate = 0\n",
    "    for xi in data:\n",
    "        estimate += gaussian_kernel((x-xi)/bandwidth)\n",
    "    return estimate/ (n*bandwidth)\n",
    "def weight_prior(weights: np.ndarray, dims: int, mean: np.ndarray[float], kernel: np.ndarray[float]):\n",
    "    return multi_n.logpdf(weights, mean=mean, cov=kernel)\n",
    "# Multivariate Posterior \n",
    "def observe_variance(y_pred: np.ndarray, y_true: np.ndarray, dim: int):\n",
    "    n = len(y_pred)\n",
    "    residuals = np.square(y_true-y_pred).mean()\n",
    "    return residuals\n",
    "def simp_three_eighth(f: callable, bounds: list, n: int):\n",
    "    if n%3 != 0:\n",
    "        raise ValueError('n must be divisible by 3')\n",
    "    a, b = bounds\n",
    "    step_size = (b-a)/n\n",
    "    fa = f(a)\n",
    "    fb = f(b)\n",
    "    sum = 0\n",
    "    for i in range(1, n):\n",
    "        multiplier = 2 if i%3 else 3\n",
    "        sum += multiplier*f(a+i*step_size)\n",
    "    sum *= (3/8)*step_size\n",
    "    return sum\n",
    "\n",
    "# def maximum_likelihood(data: np.ndarray, y_true: np.ndarray,weights:np.ndarray) -> tuple[float]:\n",
    "#     rows, columns = data.shape\n",
    "#     y_pred = data.dot(weights)\n",
    "#     obs_var = np.square(y_pred-y_true).mean()\n",
    "#     lhood = np.sum(np.log((1/np.sqrt(2*np.pi*obs_var))*np.exp(-np.square(y_true-y_pred)/(2*obs_var))))\n",
    "#     return lhood, obs_var\n",
    "def multi_likelihood(y_pred: np.ndarray, y_true: list, obs_var: float):\n",
    "    '''\n",
    "    Returns p(y|X,w)\n",
    "    '''\n",
    "    post  = np.sum(ss.norm.logpdf(y_true, loc=y_pred, scale=obs_var))\n",
    "    return post\n",
    "def marginal_likelihood(data: np.ndarray, y_true: np.ndarray, obs_var: float, dims: int, w_samp: int, w_mean: np.ndarray, w_var: np.ndarray):\n",
    "    '''\n",
    "    Novice attempt at approximating the marginal likelihood.\n",
    "    Returns p(y|X): summing over the weights\n",
    "    **This approach requires mcmc for better accuracy; thus, don't use this callable in practice.**\n",
    "    '''\n",
    "    n = len(data)\n",
    "    weight_samp = multi_n.rvs(mean=w_mean, cov=w_var, size=w_samp)\n",
    "    marg = -np.inf\n",
    "    for w in weight_samp:\n",
    "        # print(w)\n",
    "        marg = np.logaddexp(marg, multi_likelihood(data.dot(w), y_true, obs_var) + weight_prior(w, dims, w_mean, w_var))\n",
    "    return marg - np.log(w_samp)\n",
    "\n",
    "def alt_weight_post(data: np.ndarray, y: np.ndarray, w: np.ndarray, obs_var: float, w_var: float):\n",
    "    sigma_n = np.linalg.inv(data.T.dot(data)/obs_var + np.linalg.inv(w_var))\n",
    "    wn = np.dot(sigma_n, np.dot(data.T, y)/obs_var +np.dot(np.linalg.inv(w_var), w))\n",
    "    return wn, sigma_n\n",
    "\n",
    "\n",
    "def mcmc(y_true, y_pred, obs_var, dims):\n",
    "    weight_i1 = multi_n.rvs(mean=np.zeros((dims, )), cov=np.eye(dims))\n",
    "    weight = []\n",
    "    for i in count():\n",
    "        weight_i2 = uni.rvs(loc = np.zeros((dims,)), scale=1)\n",
    "        prior_i1 = multi_n.pdf()\n",
    "        lhood_i = multi_likelihood(y_pred, y_true,obs_var)\n",
    "    pass \n",
    "\n",
    "\n",
    "transformer = QuantileTransformer(n_quantiles=5000, output_distribution='normal', random_state=0)\n",
    "\n",
    "# Scaled Training Data:\n",
    "transformer.fit(val[0])\n",
    "XS = transformer.transform(val[0])\n",
    "\n",
    "# # normali = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "# # XS = val[0].copy()\n",
    "# # Performing Gradient Descent:\n",
    "# gd = Gradient_Descent(XS, val[2], 1e-10)\n",
    "# newtheta,*_ = gd.fit(1e-3)\n",
    "\n",
    "# w, obs_var = maximum_likelihood(XS, val[2], np.zeros((9,)), np.eye(9))\n",
    "# wn, sigma_n = alt_weight_post(XS, val[2], w, obs_var, np.eye(9))\n",
    "\n",
    "# w_var = np.eye(9)\n",
    "# w_mean = np.zeros((9,))\n",
    "# weight_set = list()\n",
    "# train_var = list()\n",
    "# for i in range(1000):\n",
    "#     w, obs_var = maximum_likelihood(XS, val[2],w_mean,w_var)\n",
    "#     train_var.append(obs_var)\n",
    "#     wn, sigma_n = alt_weight_post(XS, val[2], w, obs_var, w_var)\n",
    "#     w_var = sigma_n\n",
    "#     w_mean = wn\n",
    "#     weight_set.append(w)  \n",
    "\n",
    "# print(f'Initial Weights:{weight_set[0]} with model variance: {np.square(XS.dot(weight_set[0])-val[2]).mean()}')\n",
    "# best_var = min(train_var)\n",
    "# best_param = weight_set[train_var.index(best_var)]\n",
    "# print(f' Last Weight:{best_param} with model variance:{np.square(XS.dot(best_param)-val[2]).mean()}')\n",
    "# prior_w = multi_n.logpdf(newtheta, mean=np.zeros((9,)), cov=np.eye(9))\n",
    "# XBIAS = np.c_[np.ones((XS.shape[0],1)), XS]\n",
    "# print(gd.loglikelihood(8))\n",
    "# sigma2 = observe_variance(XBIAS.dot(newtheta), val[2].to_numpy(), 9)\n",
    "# w_mean, w_vari = alt_weight_post(XBIAS, val[2].to_numpy(), newtheta, sigma2, np.eye(9))\n",
    "# print(multi_n.pdf(XBIAS,mean=w_mean, cov=w_vari))\n",
    "# print(f'Observed Variance:{sigma2}')\n",
    "# print(f'Likelihood:{multi_likelihood(XBIAS.dot(newtheta), val[2].to_numpy(), sigma2)}')\n",
    "# print(f'Marginal Likelihood:{marginal_likelihood(XBIAS, val[2].to_numpy(), sigma2, 9,w_samp=5000, w_mean=np.zeros((9,)),w_var=np.eye(9))}')\n",
    "# print(f'Exponential Probability of Parameters:{(multi_likelihood(XBIAS.dot(newtheta), val[2].to_numpy(), sigma2)+prior_w)-(marginal_likelihood(XBIAS, val[2].to_numpy(), sigma2, 9,w_samp=5000, w_mean=np.zeros((9,)),w_var=np.eye(9)))}')\n",
    "# print(kernel(XBIAS.dot(newtheta)))\n",
    "# post_y = multi_likelihood(XBIAS.dot(newtheta), val[2].to_numpy(), sigma2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.randn(1, val[0].shape[1])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_likelihood(data, y_true, ideal_error):\n",
    "    '''\n",
    "    This function attempts to find all such weights (w) that\n",
    "    likely explains output distribution. I wish to compute the marginal \n",
    "    likelihood.\n",
    "    p(w|X,y)\n",
    "    '''\n",
    "\n",
    "    # w = multi_n.rvs(mean=np.zeros((data.shape[1],)), cov=np.eye(data.shape[1]),size=1, random_state=0)\n",
    "    w = ss.uniform.rvs(loc=np.zeros((data.shape[1],)), scale=2*np.eye(data.shape[1]), random_state=0)[:,0]\n",
    "    wl = [w]\n",
    "    tot,accepted = 0,0\n",
    "    scale = np.eye(data.shape[1])\n",
    "    # pot_prob = []\n",
    "    y_pred = data.dot(wl[-1])\n",
    "    pred_error = np.mean(np.square(y_true-y_pred))\n",
    "    while np.isclose(pred_error,ideal_error, atol=1e-5) != True:\n",
    "        candidate = multi_n.rvs(mean=wl[-1], cov=scale,size=1)\n",
    "        # print(candidate)\n",
    "        y_pred = data.dot(wl[-1])\n",
    "        error = np.mean(np.square(y_true-y_pred))\n",
    "        llhood_w = multi_likelihood(y_true,y_pred, error)\n",
    "\n",
    "        y_pred_u = data.dot(candidate)\n",
    "        error_u = np.mean(np.square(y_true-y_pred_u))\n",
    "        llhood_u = multi_likelihood(y_true, y_pred, error_u)\n",
    "        log_prob = llhood_u/llhood_w\n",
    "        # print(log_prob)\n",
    "        # if np.random.random()<log_prob:\n",
    "        if log_prob<=1 and log_prob>0:\n",
    "            accepted+=1\n",
    "            wl.append(candidate)\n",
    "            print(candidate)\n",
    "            # wl.pop(0) if tot>6 else None\n",
    "            # scale = kernel(np.vstack((wl[-5:-1], wl[-1]))) if tot>3 else np.eye(data.shape[1])\n",
    "            # print(scale)\n",
    "            y_pred = data.dot(wl[-1])\n",
    "            pred_error = np.mean(np.square(y_true-y_pred))\n",
    "            print(f'The Prediction error of accepted sample:\\t{pred_error} and acceptance log prob.:\\t{log_prob}')\n",
    "            \n",
    "            print(f'Percent Accepted:\\t{accepted/(tot+1)}')\n",
    "        else:\n",
    "            # wl.append(wl[-1])\n",
    "            # scale = kernel(np.vstack((wl[-2:-1], wl[-1]))) if tot>3 else np.eye(data.shape[1])\n",
    "            # print(f'Percent Accepted:\\t{accepted/tot}') if tot%20_000 else None\n",
    "            pass\n",
    "            # wl.pop()\n",
    "            # wl.append(wl[-1])\n",
    "            # tvec = np.vstack((wl[-2], wl[-1])) if len(wl) == 2 else np.vstack((wl[-3:-1], wl[-1]))\n",
    "            # scale.append(kernel(tvec))\n",
    "            # y_pred = data.dot(wl[-1])\n",
    "            # pred_error = np.mean(np.square(y_true-y_pred))\n",
    "            # print(f'Percent Accepted:\\t{accepted/len(wl)}') if tot%10_000 else None\n",
    "            # print(f'The Prediction error of unaccepted sample:\\t{pred_error} and acceptance prob.:\\t{log_prob}')  \n",
    "        tot+=1\n",
    "        \n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15242834 -0.09459476 -0.23005919 -0.26354219  0.29830636 -0.12961158\n",
      "  1.40022662  0.10563053  0.84633351]\n",
      "The Prediction error of accepted sample:\t3.2690319900333296 and acceptance log prob.:\t0.864997254393169\n",
      "Percent Accepted:\t0.5\n",
      "[ 0.99023014  0.4889536  -0.57220481 -2.26176059  1.61954746  0.55449242\n",
      "  1.99575642  1.02393797  0.68065436]\n",
      "The Prediction error of accepted sample:\t3.150645950880551 and acceptance log prob.:\t0.9888422352962656\n",
      "Percent Accepted:\t0.4\n",
      "[ 0.41651116  0.85969267 -0.84296088 -1.92362344  2.59573082 -0.47378799\n",
      "  0.99601799  1.9290039   1.09717308]\n",
      "The Prediction error of accepted sample:\t2.680744967440082 and acceptance log prob.:\t0.9546111421057849\n",
      "Percent Accepted:\t0.1875\n",
      "[-0.10310646  1.85555749 -0.46828733 -1.99276808  2.43067528 -0.2951857\n",
      "  1.05531214  0.73367109  0.59487767]\n",
      "The Prediction error of accepted sample:\t1.8901770672183504 and acceptance log prob.:\t0.9231306007045597\n",
      "Percent Accepted:\t0.07692307692307693\n",
      "[-0.34506033  4.29335156 -1.16304523 -2.59170231  2.81597073 -1.07905066\n",
      "  1.88964234  0.31338662 -1.08075554]\n",
      "The Prediction error of accepted sample:\t1.840676032381944 and acceptance log prob.:\t0.9933419233661017\n",
      "Percent Accepted:\t0.07936507936507936\n",
      "[ 0.16321502  4.69740128 -0.50940563 -3.5252598   2.23307062 -0.18676525\n",
      "  1.88037659 -0.16288867 -0.11514522]\n",
      "The Prediction error of accepted sample:\t1.6240383230964934 and acceptance log prob.:\t0.9733920953923818\n",
      "Percent Accepted:\t0.08571428571428572\n",
      "[-1.92812702  4.30400568  0.30223873 -3.50364196  2.57458608 -0.63740176\n",
      "  1.18249231 -0.2029034  -0.16022807]\n",
      "The Prediction error of accepted sample:\t1.4012683705890971 and acceptance log prob.:\t0.9755412675866588\n",
      "Percent Accepted:\t0.09210526315789473\n",
      "[-2.55962935  5.2402293   0.15413034 -1.98154179  0.97677775  1.13420777\n",
      "  0.56591931 -0.22258389 -0.78542831]\n",
      "The Prediction error of accepted sample:\t1.3400964998535962 and acceptance log prob.:\t0.9929845125507177\n",
      "Percent Accepted:\t0.1038961038961039\n",
      "[-2.12850728  4.60773213 -0.30914819 -1.30160392  0.49078557  1.25294298\n",
      "  0.2737963   0.06564973 -0.41488469]\n",
      "The Prediction error of accepted sample:\t1.1922087530196839 and acceptance log prob.:\t0.9882454275668016\n",
      "Percent Accepted:\t0.0891089108910891\n",
      "[-0.59872021  4.35835798  1.16566244 -1.48343956 -0.11531979  0.68416324\n",
      " -0.41766122 -0.16242929  0.96108936]\n",
      "The Prediction error of accepted sample:\t1.1241601457343142 and acceptance log prob.:\t0.9957328589562066\n",
      "Percent Accepted:\t0.04975124378109453\n",
      "[-0.71100766  4.04557412  2.03744447 -0.94678277 -0.73805885  0.09506732\n",
      "  0.01242892 -0.56583287  0.11316701]\n",
      "The Prediction error of accepted sample:\t1.079759250559299 and acceptance log prob.:\t0.997996541092729\n",
      "Percent Accepted:\t0.03691275167785235\n",
      "[-0.44727036  4.29347145  2.06449219 -1.181869   -0.81547779  0.62641117\n",
      " -0.58093632 -0.29380132 -0.23483123]\n",
      "The Prediction error of accepted sample:\t1.0541734175898503 and acceptance log prob.:\t0.9991566730134372\n",
      "Percent Accepted:\t0.03468208092485549\n",
      "[-0.10857947  3.79599221  1.3556057  -0.12137594  0.23934137  0.47545744\n",
      " -1.9861596  -0.33166358 -0.26062785]\n",
      "The Prediction error of accepted sample:\t1.0173014518114396 and acceptance log prob.:\t0.999586361744533\n",
      "Percent Accepted:\t0.031862745098039214\n",
      "[-1.39682128  4.51921006  1.37135157  0.02011421 -0.24272127  0.59065324\n",
      " -1.13387884 -0.47303134 -0.9172534 ]\n",
      "The Prediction error of accepted sample:\t1.0130568524470145 and acceptance log prob.:\t0.9999622602182286\n",
      "Percent Accepted:\t0.01734820322180917\n",
      "[ 0.2628752   4.11669307  2.39256264 -0.48134311  0.47628903 -0.21388932\n",
      " -0.22749415 -1.63029332 -0.78502526]\n",
      "The Prediction error of accepted sample:\t1.003241848191471 and acceptance log prob.:\t0.9999780377137084\n",
      "Percent Accepted:\t0.017462165308498253\n",
      "[-0.18148462  4.30137536  2.29144554 -0.44189162  0.41131232 -0.03155985\n",
      " -0.04345677 -1.72162702 -1.07773128]\n",
      "The Prediction error of accepted sample:\t1.0031805140506826 and acceptance log prob.:\t0.9999998635515351\n",
      "Percent Accepted:\t0.001984865401314973\n",
      "[ 0.68297044  3.90382202  2.18628974  0.51552755  0.68634734  0.7527009\n",
      " -0.70882272 -1.37296378 -2.20027116]\n",
      "The Prediction error of accepted sample:\t1.0019565002165076 and acceptance log prob.:\t0.9999983219759953\n",
      "Percent Accepted:\t0.002012310606060606\n",
      "[ 1.07291668  3.84726152  0.9676487   0.53046823  0.43152419  1.62872822\n",
      " -0.79853964 -1.65132864 -1.381446  ]\n",
      "The Prediction error of accepted sample:\t1.0008186994307715 and acceptance log prob.:\t0.9999993455807028\n",
      "Percent Accepted:\t0.0011945052757316346\n",
      "[ 1.79760106  4.6365595   1.16953185  0.02991716  0.91028435  1.47846444\n",
      " -1.16066773 -1.24243902 -1.49700598]\n",
      "The Prediction error of accepted sample:\t1.0007145990409503 and acceptance log prob.:\t0.9999999476662597\n",
      "Percent Accepted:\t0.0004965762375202551\n",
      "[ 3.62271131  4.93319826  0.81226966 -0.22673818  1.94561461  0.3182522\n",
      " -0.7048725  -1.60797123 -1.2690152 ]\n",
      "The Prediction error of accepted sample:\t1.000232910346096 and acceptance log prob.:\t0.9999999210224915\n",
      "Percent Accepted:\t0.0005034359503612153\n",
      "[ 5.46034131  3.38994694  0.7667231  -0.71739339  1.77622476  1.65433589\n",
      "  0.17545384 -1.39060263 -1.37960317]\n",
      "The Prediction error of accepted sample:\t1.0001809775374206 and acceptance log prob.:\t0.9999999933793943\n",
      "Percent Accepted:\t0.00034646033029218156\n",
      "[ 4.93518787  4.15923505  0.59264406 -1.456635    2.20823957 -0.09651151\n",
      "  0.01549967 -0.6072571  -1.49479873]\n",
      "The Prediction error of accepted sample:\t1.0000804142556678 and acceptance log prob.:\t0.999999994302551\n",
      "Percent Accepted:\t0.00023599326346502471\n",
      "[ 3.7742116   4.23050373  1.55656231 -1.35384194  2.41401952  0.06491859\n",
      " -0.35882796 -1.63323244 -0.43191544]\n",
      "The Prediction error of accepted sample:\t1.0000653991414037 and acceptance log prob.:\t0.9999999993080665\n",
      "Percent Accepted:\t0.00013664448669201522\n",
      "[ 2.84754225  5.08947285  1.72315033 -1.6206346   0.67768215 -1.08908321\n",
      " -0.14384081 -0.56019114  0.02154319]\n",
      "The Prediction error of accepted sample:\t1.000059490244798 and acceptance log prob.:\t0.9999999997523001\n",
      "Percent Accepted:\t0.0001161192932205686\n",
      "[ 2.86255237  5.08630377  1.37087447 -2.06254401  0.30541861  0.26791793\n",
      " -1.22850996  0.89749965 -0.45846385]\n",
      "The Prediction error of accepted sample:\t1.0000320242524066 and acceptance log prob.:\t0.999999999380179\n",
      "Percent Accepted:\t9.188541521181426e-05\n",
      "[ 3.31488149  5.53981469  1.63351072 -2.69484442  0.6479084   0.68792863\n",
      " -0.73758156 -0.25796874 -0.87665821]\n",
      "The Prediction error of accepted sample:\t1.0000016385408286 and acceptance log prob.:\t0.999999999964916\n",
      "Percent Accepted:\t7.237723012340318e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wl \u001b[38;5;241m=\u001b[39m \u001b[43mdependent_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mdependent_likelihood\u001b[1;34m(data, y_true, ideal_error)\u001b[0m\n\u001b[0;32m     24\u001b[0m y_pred_u \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdot(candidate)\n\u001b[0;32m     25\u001b[0m error_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msquare(y_true\u001b[38;5;241m-\u001b[39my_pred_u))\n\u001b[1;32m---> 26\u001b[0m llhood_u \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m llhood_u\u001b[38;5;241m/\u001b[39mllhood_w\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print(log_prob)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# if np.random.random()<log_prob:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m, in \u001b[0;36mmulti_likelihood\u001b[1;34m(y_pred, y_true, obs_var)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_likelihood\u001b[39m(y_pred: np\u001b[38;5;241m.\u001b[39mndarray, y_true: \u001b[38;5;28mlist\u001b[39m, obs_var: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Returns p(y|X,w)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     post  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_var\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2028\u001b[0m, in \u001b[0;36mrv_continuous.logpdf\u001b[1;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[0;32m   2026\u001b[0m cond1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_support_mask(x, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m&\u001b[39m (scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2027\u001b[0m cond \u001b[38;5;241m=\u001b[39m cond0 \u001b[38;5;241m&\u001b[39m cond1\n\u001b[1;32m-> 2028\u001b[0m output \u001b[38;5;241m=\u001b[39m empty(\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m, dtyp)\n\u001b[0;32m   2029\u001b[0m output\u001b[38;5;241m.\u001b[39mfill(NINF)\n\u001b[0;32m   2030\u001b[0m putmask(output, (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mcond0)\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbadvalue)\n",
      "File \u001b[1;32mc:\\Users\\dwigh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1976\u001b[0m, in \u001b[0;36m_shape_dispatcher\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    Return the indices of the elements that are non-zero.\u001b[39;00m\n\u001b[0;32m   1885\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \n\u001b[0;32m   1972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonzero\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape_dispatcher\u001b[39m(a):\n\u001b[0;32m   1977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m   1980\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_shape_dispatcher)\n\u001b[0;32m   1981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(a):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wl = dependent_likelihood(val[0], val[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.square(val[0].dot(wl[-1])-val[2]).mean())\n",
    "g = multi_n.rvs(mean=np.mean(np.array(wl), axis=0), cov=kernel(np.array(wl)))\n",
    "print(np.square(val[0].dot(g)-val[2]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.square(val[1].dot(g)-val[3]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.random.normal(loc=0, scale=1, size=(1000, 3)), np.ones(shape=(1000,))]\n",
    "weight = np.random.randn(4,1)\n",
    "print(f'Model Weights:\\t{weight}')\n",
    "y = X.dot(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dependent_likelihood(X, y)\n",
    "[np.mean(np.square(y - X.dot(weights[i]))) for i in range(len(weights))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
